{"cells":[{"cell_type":"markdown","metadata":{"id":"Uk0ZyxjPoGhL"},"source":["# DL Lab 3.1 - Introduction to Natural Language Processing\n","\n","In this section, we will explore different neural network architectures for dealing with sequential data such as text, i.e., natural language. In recent years, **Natural Language Processing (NLP)** has experienced fast growth as a field, both because of improvements to the language model architectures and because they've been trained on increasingly large text corpora. As a result, their ability to \"understand\" text has vastly improved, and large pre-trained models such as BERT and GPT have become widely used.\n","\n","We will focus on the fundamental aspects of **representing NLP as tensors** in TensorFlow, and on classical NLP architectures, such as using **bag-of-words**, **embeddings**, **recurrent neural networks**, and **Transformers**.\n","\n","## Today's Learning Objectives\n","\n","* Understand how **text** is processed for NLP tasks\n","* Learn how to build **text classification** models\n","* Learn about **Recurrent Neural Networks** (RNNs)\n","\n","***\n","\n","**Instructions**\n","\n","- You'll be using Python 3 in the iPython based Google Colaboratory\n","- Lines encapsulated in \"<font color='green'>`### START YOUR CODE HERE ###`</font>\" and \"<font color='green'>`### END YOUR CODE HERE ###`</font>\", or marked by \"<font color='green'>`# TODO`</font>\", denote the code fragments to be completed by you.\n","- There's no need to write any other code.\n","- After writing your code, you can run the cell by either pressing `SHIFT`+`ENTER` or by clicking on the play symbol on the left side of the cell.\n","- We may specify \"<font color='green'>`(≈ X LOC)`</font>\" in the \"<font color='green'>`# TODO`</font>\" comments to tell you about how many lines of code you need to write. This is just a rough estimate, so don't feel bad if your code is longer or shorter.\n","- If you get stuck, check your Lecture and Lab notes and use the [discussion forum](https://moodle.tu-ilmenau.de/mod/forum/view.php?id=3371) in Moodle.\n","\n","Let's get started!\n","\n","***\n","\n","**Note**: Training DNNs is a computationally expensive process. Most of the computations can be parallelized very efficently, making them a perfect fit for GPU-acceleration. In order to enable a GPU for your Colab session, do the following steps:\n","- Click '*Runtime*' -> '*Change runtime type*'\n","- In the pop-up window for '*Hardware accelerator*', select '*GPU*'\n","- Click '*Save*'"]},{"cell_type":"markdown","metadata":{"id":"_55KyvBa1Uq4"},"source":["## Natural Language Tasks\n","\n","There are several NLP tasks that we can solve using neural networks:\n","\n","* **Text Classification** is used when we need to classify a text fragment into one of several predefined classes. Examples include e-mail spam detection, news categorization, assigning a support request to a category, and more.\n","* **Intent Classification** is one specific case of text classification, where we want to map an input utterance in the conversational AI system into one of the intents that represent the actual meaning of the phrase, or intent of the user.\n","* **Sentiment Analysis** is a regression task, where we want to understand the degree of positivity of a given piece of text. We may want to label text in a dataset from most negative (-1) to most positive (+1), and train a model that will output a number representing the positivity of the input text.\n","* **Named Entity Recognition (NER)** is the task of extracting entities from text, such as dates, addresses, people names, etc. Together with intent classification, NER is often used in dialog systems to extract parameters from the user's utterance.\n","* A similar task of **Keyword Extraction** can be used to find the most meaningful words inside a text, which can then be used as tags.\n","* **Text Summarization** extracts the most meaningful pieces of text, giving the user a compressed version of the original text.\n","* **Question Answering** is the task of extracting an answer from a piece of text. This model takes a text fragment and a question as input, and finds the exact place within the text that contains the answer. For example, the text \"John is a 22 year old student who loves to use Microsoft Learn\", and the question How old is John should provide us with the answer 22.\n","\n","In this Lab, we focus on the **Text Classification** task. However, we will learn all the important concepts that we need to handle more difficult tasks in the future."]},{"cell_type":"markdown","metadata":{"id":"Qus3xOo92m5c"},"source":["# 1 - Download a Text Dataset\n","\n","In this module, we will start with a simple text classification task based on the **[AG_NEWS](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)** dataset: we'll classify news headlines into one of 4 categories: World, Sports, Business and Sci/Tech.\n","\n","To load the dataset, we will use the **[TensorFlow Datasets](https://www.tensorflow.org/datasets)** API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YYVvz_YFp1wN"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_datasets as tfds\n","\n","dataset = tfds.load('ag_news_subset')"]},{"cell_type":"markdown","metadata":{"id":"PPObbq6-3Iq6"},"source":["Once you've acquired a new dataset to work with, what should you do first?\n","\n","Explore it? Inspect it? Verify it? Become one with it?\n","\n","**All correct.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAs90Ire2mNK"},"outputs":[],"source":["dataset.keys()"]},{"cell_type":"markdown","metadata":{"id":"fKSEyGSd3coH"},"source":["We can access the training and test portions of the dataset by using `dataset['train']` and `dataset['test']` respectively:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51F5w48q3gGU"},"outputs":[],"source":["ds_train = dataset['train']\n","ds_test = dataset['test']\n","\n","print(f\"Length of train dataset = {len(ds_train)}\")\n","print(f\"Length of test dataset = {len(ds_test)}\")"]},{"cell_type":"markdown","metadata":{"id":"rub6PLm53kHm"},"source":["Let's print out the first 10 new headlines from our dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-itl3vHG3lIe"},"outputs":[],"source":["classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n","num_classes = len(classes)\n","\n","for i,x in zip(range(10), ds_train):\n","    print(f\"{x['label']} ({classes[x['label']]}) -> {x['title']} {x['description']}\")"]},{"cell_type":"markdown","metadata":{"id":"zfcoMB735xDo"},"source":["Let's check how many examples of each label we have in the train split:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qr9GmR9r5wbX"},"outputs":[],"source":["samples_per_class = np.zeros(num_classes)\n","for x in ds_train:\n","  samples_per_class[x['label']] += 1\n","\n","for class_idx, samples in zip(range(num_classes), samples_per_class):\n","  print(f\"label: {class_idx}, samples: {samples:.0f}\")"]},{"cell_type":"markdown","metadata":{"id":"nZiHSlWFKXz-"},"source":["Wonderful! We've got a training set and a validation set containing text and labels. Our labels are in numerical form (`0`, `1`, `2`, `3`) but our texts are in string form. If we want to solve Natural Language Processing (NLP) tasks with neural networks, we need some way to represent text as tensors."]},{"cell_type":"markdown","metadata":{"id":"LHAUywehi-R4"},"source":["Finally, let us define two helper functions to better deal with this dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysRgHSmqiFRb"},"outputs":[],"source":["def extract_text(x):\n","    return x['title'] + ' ' + x['description']\n","\n","def tupelize(x):\n","    return (extract_text(x), x['label'])"]},{"cell_type":"markdown","metadata":{"id":"G7_8wBrcpxvT"},"source":["# 2 - Representing Text as Tensors"]},{"cell_type":"markdown","metadata":{"id":"tiry37s40Nyj"},"source":["In NLP, there are two main concepts for turning text into numbers:\n","\n","* **Tokenization** - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n","  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being `0`, \"love\" being `1` and \"TensorFlow\" being `2`. In this case, every word in a sequence considered a single **token**.\n","  2. **Character-level tokenization**, such as converting the letters a-z to values `1-26`. In this case, every character in a sequence considered a single **token**.\n","  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n","\n","* **Embeddings** - An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector `[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:\n","  1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.\n","  2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task."]},{"cell_type":"markdown","metadata":{"id":"DbTOIerpN31a"},"source":["\n","> 🤔 **Question:** *What level of tokenzation should we use? What embedding should I choose?*\n","\n","It depends on your problem. You could try character-level tokenization/embeddings and word-level tokenization/embeddings and see which perform best.\n","\n","If you're looking for pre-trained word embeddings, [Word2vec embeddings](http://jalammar.github.io/illustrated-word2vec/), [GloVe embeddings](https://nlp.stanford.edu/projects/glove/) and many of the options available on [TensorFlow Hub](https://tfhub.dev/s?module-type=text-embedding) are great places to start.\n","\n","> ⭐ **Note:** Much like searching for a pre-trained computer vision model, you can search for pre-trained word embeddings to use for your problem. Try searching for something like \"use pre-trained word embeddings in TensorFlow\"."]},{"cell_type":"markdown","metadata":{"id":"jHKDCB945C1f"},"source":["We'll practice **word-level tokenzation** (mapping our words to numbers) first.\n","Therefore, we need to do two things:\n","\n","* Use a **tokenizer** to split text into **tokens**.\n","* Build a **vocabulary** of those tokens.\n","\n","### Limiting vocabulary size\n","\n","In the AG News dataset example, the vocabulary size is rather big, more than 100k words. Generally speaking, we don't need words that are rarely present in the text &mdash; only a few sentences will have them, and the model will not learn from them. Thus, it makes sense to limit the vocabulary size to a smaller number by passing an argument to the vectorizer constructor:\n","\n","Both of those steps can be handled using the **[TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization)** layer. Let's instantiate the vectorizer object, and then call the `adapt` method to go through all text and build a vocabulary:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyACuuq35nH_"},"outputs":[],"source":["from tensorflow.keras import layers\n","\n","vocab_size = 50000\n","\n","vectorizer = layers.TextVectorization(max_tokens=vocab_size)\n","vectorizer.adapt(ds_train.take(1000).map(extract_text))"]},{"cell_type":"markdown","metadata":{"id":"MmQ0tq3SR3Gr"},"source":["> ⭐ **Note:** We are using only subset of the whole dataset (`ds_train.take(1000)`) to build a vocabulary. We do it to speed up the execution time and not keep you waiting. However, we are taking the risk that some of the words from the whole dateset would not be included into the vocabulary, and will be ignored during training. Thus, using the whole vocabulary size and running through all dataset during adapt should increase the final accuracy, but not significantly.\n","\n","Now we can access the actual vocabulary:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U8bU7W1AX1vm"},"outputs":[],"source":["# Get the unique words in the vocabulary\n","vocab = vectorizer.get_vocabulary()\n","\n","# Length of the vocabulary\n","vocab_size = len(vocab)\n","print(f\"Number of words in vocab: {vocab_size}\")\n","\n","# most common tokens (notice the [UNK] token for \"unknown\" words)\n","top_5_words = vocab[:5]\n","print(f\"Top 5 most common words: {top_5_words}\")\n","\n","# least common tokens\n","bottom_5_words = vocab[-5:]\n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"]},{"cell_type":"markdown","metadata":{"id":"x9jJODsmSoo8"},"source":["Using the vectorizer, we can easily encode any text into a set of numbers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mr9KxJPPSpOf"},"outputs":[],"source":["# Create sample sentence and tokenize it\n","sample_sentence = \"I like deep learning\"\n","vectorizer([sample_sentence])"]},{"cell_type":"markdown","metadata":{"id":"FsBTjWylWLBS"},"source":["# 3 - BoW as a Simple Language Model"]},{"cell_type":"markdown","metadata":{"id":"fGRliZNoWhaM"},"source":["Because words represent meaning, sometimes we can figure out the meaning of a piece of text by just looking at the individual words, regardless of their order in the sentence. For example, when classifying news, words like *weather* and *snow* are likely to indicate *weather forecast*, while words like *stocks* and *dollar* would count towards *financial news*.\n","\n","**Bag-of-words** (BoW) vector representation is the most simple to understand traditional vector representation. Each word is linked to a vector index, and a vector element contains the number of occurrences of each word in a given document.\n","\n","> ⭐ **Note**: BoW is essentially the sum of all one-hot-encoded vectors for individual words in the text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6IyJIoLUeBm"},"outputs":[],"source":["from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","\n","def build_bow_model(vectorizer, vocab_size, num_classes):\n","\n","  input = layers.Input(shape=(1,), dtype=tf.string)\n","  x = vectorizer(input)\n","  x = tf.one_hot(x, vocab_size)\n","  x = tf.reduce_sum(x, axis=1)\n","  output = layers.Dense(num_classes, activation='softmax')(x)\n","\n","  model = tf.keras.models.Model(input, output)\n","  model.compile(\n","      loss=SparseCategoricalCrossentropy(),\n","      optimizer=Adam(),\n","      metrics=[SparseCategoricalAccuracy()]\n","  )\n","\n","  print(model.summary())\n","\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLu8yws9dmgb"},"outputs":[],"source":["bow_model = build_bow_model(vectorizer, vocab_size, num_classes)"]},{"cell_type":"markdown","metadata":{"id":"LxhdZZI6yCYI"},"source":["In the model `summary`, in the *Output Shape* column, the first tensor dimension `None` corresponds to the minibatch size, and the second corresponds to the length of the token sequence. All token sequences in the minibatch have different lengths. We'll discuss how to deal with it when implementing RNNs.\n","\n","Here is an example computation of a BoW vector:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wlUzG4nCyN4o"},"outputs":[],"source":["sample_sentence = tf.convert_to_tensor([\"I like deep learning\"], dtype=tf.string)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqpL-kAwxJVj"},"outputs":[],"source":["for layer_idx, layer in enumerate(bow_model.layers[1:-1], start=1):\n","\n","  # define model with auxiliary output\n","  aux_output_model = tf.keras.models.Model(bow_model.input, layer.output)\n","\n","  # get auxiliary output\n","  out = aux_output_model(sample_sentence).numpy()\n","\n","  print(f\"Layer {layer_idx} - {layer.name}: output shape: {out.shape}\")\n","  print(f\"output = {out}\")\n","  if layer_idx == 1:\n","    indices = out # get word_indices\n","  elif layer_idx == 3:\n","    print(f\"output[indices] = {out[0,indices]}\\n\")\n","  print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EkBuoE01h_w7"},"outputs":[],"source":["# optimize the datasets for training\n","BATCHSIZE = 128\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","ds_train_opt = ds_train.map(tupelize).cache().shuffle(1000).batch(BATCHSIZE).prefetch(AUTOTUNE)\n","ds_test_opt = ds_test.map(tupelize).cache().batch(1000).prefetch(AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"M0Eu1oBE3B2m"},"source":["Now that we have learned how to build the bag-of-words representation of our text, let's train a classifier that uses it:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fDHt6CFgXLc"},"outputs":[],"source":["bow_history = bow_model.fit(\n","    ds_train_opt,\n","    validation_data=ds_test_opt,\n","    epochs=5\n",")"]},{"cell_type":"markdown","metadata":{"id":"WVMImBAk8lm8"},"source":["Since we have 4 classes, an accuracy of above 80% is a good result."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Hcl1dm4fUeKn"},"outputs":[],"source":["# @title define `plot_history()`\n","from matplotlib import pyplot as plt\n","\n","def plot_history(history):\n","  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n","  ax1.plot(history.history['loss'], label='training')\n","  ax1.plot(history.history['val_loss'], label='validation')\n","  ax1.set_ylabel('Loss')\n","  ax1.set_yscale('log')\n","  if history.history.__contains__('lr'):\n","    ax1b = ax1.twinx()\n","    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n","    ax1b.set_yscale('log')\n","    ax1b.set_ylabel('Learning Rate', color='g')\n","  ax1.legend()\n","\n","  key = None\n","  for k in sorted(history.history.keys()):\n","    if 'acc' in k and not 'val_' in k:\n","      key = k\n","      break\n","  if key:\n","    ax2.plot(history.history[key], label='training')\n","    ax2.plot(history.history['val_'+key], label='validation')\n","    ax2.set_ylabel('Accuracy')\n","    ax2.set_xlabel('Epochs')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nr_Fy7Z9pLCp"},"outputs":[],"source":["plot_history(bow_history)"]},{"cell_type":"markdown","metadata":{"id":"8m-oRQbD8xVm"},"source":["# 4 - Addition of Embedding Layer\n"]},{"cell_type":"markdown","metadata":{"id":"8srCw4tI9GZr"},"source":["When training the classifier based on BoW, we operated on high-dimensional bag-of-words vectors with length `vocab_size`, and we were explicitly converting from low-dimensional positional representation vectors into sparse one-hot representation. This one-hot representation, however, is not memory-efficient. In addition, each word is treated independently from each other, i.e. one-hot encoded vectors do not express any semantic similarity between words.\n","\n","The idea of **embedding** is to represent words using lower-dimensional dense vectors that reflect the semantic meaning of the word.\n","So, an embedding layer takes a word as input, and produces an output vector of specified `embedding_size`. In a sense, it is very similar to a `Dense` layer, but instead of taking a one-hot encoded vector as input, it's able to take a word number.\n","\n","By using an embedding layer as the first layer in our network, we can switch from bag-of-words to an **embedding bag** model, where we first convert each word in our text into the corresponding embedding, and then compute some aggregate function over all those embeddings, such as `sum`, `average` or `max`.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3w2T5-bUeN6"},"outputs":[],"source":["def build_embedding_bag_model(vectorizer, vocab_size, embedding_size, num_classes):\n","\n","  embedding_layer = layers.Embedding(\n","      input_dim=vocab_size, # set input shape\n","      output_dim=embedding_size, # set size of embedding vector\n","  )\n","\n","  input = layers.Input(shape=(1,), dtype=tf.string)\n","  x = vectorizer(input)\n","  x = embedding_layer(x)\n","  x = tf.reduce_sum(x,axis=1)(x)\n","  # or: layers.Lambda(lambda x: tf.reduce_sum(x,axis=1))(x)\n","  output = layers.Dense(num_classes, activation='softmax')(x)\n","\n","  model = tf.keras.models.Model(input, output)\n","  model.compile(\n","      loss=SparseCategoricalCrossentropy(),\n","      optimizer=Adam(),\n","      metrics=[SparseCategoricalAccuracy()]\n","  )\n","\n","  print(model.summary())\n","\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91vU8XyqoPFw"},"outputs":[],"source":["emb_bag_model = build_embedding_bag_model(vectorizer, vocab_size, 128, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xy6FLPHIoUaz"},"outputs":[],"source":["emb_bag_history = emb_bag_model.fit(\n","    ds_train_opt,\n","    validation_data=ds_test_opt,\n","    epochs=5\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JF82gFFspPmI"},"outputs":[],"source":["plot_history(emb_bag_history)"]},{"cell_type":"markdown","metadata":{"id":"Rnt9ZK_BAL7T"},"source":["We will later discuss how to build meaningful word embeddings and discuss their cool properties, but for now let's just think of embeddings as a way to reduce the dimensionality of a word vector."]},{"cell_type":"markdown","metadata":{"id":"OUzJbv-8p09Y"},"source":["# 5 - A simple RNN\n","\n","In previous sections, we have been using rich semantic representations of text and a simple linear classifier on top of the embeddings. What this architecture does is to capture the aggregated meaning of words in a sentence, but it does not take into account the order of words, because the aggregation operation on top of embeddings removed this information from the original text. Because these models are unable to model word ordering, they cannot solve more complex or ambiguous tasks such as text generation or question answering.\n","\n","To capture the meaning of a text sequence, we'll use a neural network architecture called **recurrent neural network**, or RNN. When using an RNN, we pass our sentence through the network one token at a time, and the network produces some **state**, which we then pass to the network along with the next token.\n","\n","Because state vectors are passed through the network, the RNN is able to learn sequential dependencies between words. For example, when the word *not* appears somewhere in the sequence, it can learn to negate certain elements within the state vector.\n","\n","Let's see how recurrent neural networks can help us classify our news dataset."]},{"cell_type":"markdown","metadata":{"id":"qFQ8E7jZBrns"},"source":["In the case of a simple RNN, each recurrent unit is a simple linear network, which takes in an input vector and state vector, and produces a new state vector. In Keras, this can be represented by the [`SimpleRNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN) layer.\n","\n","While we can pass one-hot encoded tokens to the RNN layer directly, this is not a good idea because of their high dimensionality. Therefore, we will use an embedding layer as before to lower the dimensionality of word vectors, followed by an RNN layer, and finally a `Dense` classifier.\n","\n","> ⭐ **Note:** In cases where the dimensionality isn't so high, for example when using character-level tokenization, it might make sense to pass one-hot encoded tokens directly into the RNN cell."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIqtD9PJp1lh"},"outputs":[],"source":["def build_RNN_model(embedding_size, hidden_size, num_classes, max_vocab_size = 10000):\n","\n","  vectorizer = layers.TextVectorization(max_tokens=max_vocab_size)\n","  # It's a new vectorizer, so we need to train it first:\n","  print('Training vectorizer')\n","  vectorizer.adapt(ds_train.take(1000).map(extract_text))\n","\n","  embedding_layer = layers.Embedding(vocab_size, embedding_size)\n","\n","  input = layers.Input(shape=(1,), dtype=tf.string)\n","  x = vectorizer(input)\n","  x = embedding_layer(x)\n","  x = layers.SimpleRNN(hidden_size)(x)\n","  output = layers.Dense(num_classes, activation='softmax')(x)\n","\n","  model = tf.keras.models.Model(input, output)\n","  model.compile(\n","      loss=SparseCategoricalCrossentropy(),\n","      optimizer=Adam(),\n","      metrics=[SparseCategoricalAccuracy()]\n","  )\n","\n","  print(model.summary())\n","\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xO-wORSIqIFE"},"outputs":[],"source":["rnn_model = build_RNN_model(128, 16, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lm4ML6fQqKxa"},"outputs":[],"source":["rnn_model_history = rnn_model.fit(\n","    ds_train_opt,\n","    validation_data=ds_test_opt,\n","    epochs=5\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9H0ORlFqRAd"},"outputs":[],"source":["plot_history(rnn_model_history)"]},{"cell_type":"markdown","metadata":{"id":"OAkD46EOGL42"},"source":["# 6 - LSTM: Long short-term memory\n","\n","One of the main problems of RNNs is **vanishing gradients**. Sequences can be pretty long, and RNNs may have a hard time propagating the gradients all the way back to the first state of the network during backpropagation. When this happens, the network cannot learn relationships between distant tokens. One way to avoid this problem is to introduce **explicit state management** by using **gates**. The two most common architectures that introduce gates are **long short-term memory** (LSTM) and **gated relay unit** (GRU). We'll cover LSTMs here.\n","\n","\n","An LSTM network is organized in a manner similar to an RNN, but there are **two states** that are passed across time: the actual state $c$, and the hidden vector $h$. At each unit, the hidden vector $h_{t-1}$ is combined with input $x_t$, and together they control what happens to the state $c_t$ and output $h_{t}$ through **gates**. Each gate has sigmoid activation (output in the range $[0,1]$), which can be thought of as a bitwise mask when multiplied by the state vector. LSTMs have the following gates (from left to right on the picture above):\n","* **forget gate** which determines which components of the vector $c_{t-1}$ we need to forget, and which to pass through.\n","* **input gate** which determines how much information from the input vector and previous hidden vector should be incorporated into the state vector.\n","* **output gate** which takes the new state vector and decides which of its components will be used to produce the new hidden vector $h_t$.\n","\n","The components of the state $c$ can be thought of as flags that can be switched on and off. For example, when we encounter the name *Alice* in the sequence, we guess that it refers to a woman, and raise the flag in the state that says we have a female noun in the sentence. When we further encounter the words *and Tom*, we will raise the flag that says we have a plural noun. Thus by manipulating state we can keep track of the grammatical properties of the sentence.\n","\n","> ⭐ **Note:** Here's a great resource for understanding the internals of LSTMs: [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Christopher Olah.\n","\n","While the internal structure of an LSTM cell may look complex, Keras hides this implementation inside the [`LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) layer, so the only thing we need to do in the example above is to replace the recurrent layer:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6c4FcJoGQsC"},"outputs":[],"source":["def build_lstm_model(embedding_size, hidden_size, num_classes, max_vocab_size = 10000):\n","\n","  vectorizer = layers.TextVectorization(max_tokens=max_vocab_size)\n","  # It's a new vectorizer, so we need to train it first:\n","  print('Training vectorizer')\n","  vectorizer.adapt(ds_train.take(1000).map(extract_text))\n","\n","  embedding_layer = layers.Embedding(vocab_size, embedding_size)\n","\n","  input = layers.Input(shape=(1,), dtype=tf.string)\n","  x = vectorizer(input)\n","  x = embedding_layer(x)\n","  x = layers.LSTM(hidden_size)(x)\n","  output = layers.Dense(num_classes, activation='softmax')(x)\n","\n","  model = tf.keras.models.Model(input, output)\n","  model.compile(\n","      loss=SparseCategoricalCrossentropy(),\n","      optimizer=Adam(),\n","      metrics=[SparseCategoricalAccuracy()]\n","  )\n","\n","  print(model.summary())\n","\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"grUs0KwuHqv0"},"outputs":[],"source":["lstm_model = build_lstm_model(128, 16, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uyEc0qkzIIvv"},"outputs":[],"source":["lstm_model_history = lstm_model.fit(\n","    ds_train_opt,\n","    validation_data=ds_test_opt,\n","    epochs=5\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rC4dHKlYINRD"},"outputs":[],"source":["plot_history(lstm_model_history)"]},{"cell_type":"markdown","metadata":{"id":"h7FFLw04JFcQ"},"source":["# 7 - Bidirectional and multilayer RNNs\n","\n","In our examples so far, the recurrent networks operate from the beginning of a sequence until the end. This feels natural to us because it follows the same direction in which we read or listen to speech. However, for scenarios which require random access of the input sequence, it makes more sense to run the recurrent computation in both directions. RNNs that allow computations in both directions are called **bidirectional** RNNs, and they can be created by wrapping the recurrent layer with a special [`Bidirectional` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional).\n","\n","> ⭐ **Note:** The `Bidirectional` layer is actually a wrapper that creates two copies of the layer within it, and sets the `go_backwards` property of one of those copies to `True`, making it go in the opposite direction along the sequence.\n","\n","Recurrent networks, unidirectional or bidirectional, capture patterns within a sequence, and store them into state vectors or return them as output. As with convolutional networks, we can build another recurrent layer following the first one to capture higher level patterns, built from lower level patterns extracted by the first layer. This leads us to the notion of a **multi-layer RNN**, which consists of two or more recurrent networks, where the output of the previous layer is passed to the next layer as input.\n","\n","Keras makes constructing these networks an easy task, because you just need to add more recurrent layers to the model. For all layers except the last one, we need to specify `return_sequences=True` parameter, because we need the layer to return all intermediate states, and not just the final state of the recurrent computation.\n","\n","Let's build a two-layer bidirectional LSTM for our classification problem.\n","\n","> ⭐ **Note:** This code again takes quite a long time to complete, but it gives us highest accuracy we have seen so far. So maybe it is worth waiting and seeing the result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i2L05qhRJaZd"},"outputs":[],"source":["def build_bidirectional_two_layer_LSTM_model(embedding_size, hidden_size, num_classes, max_vocab_size = 10000):\n","\n","  vectorizer = layers.TextVectorization(max_tokens=max_vocab_size)\n","  # It's a new vectorizer, so we need to train it first:\n","  print('Training vectorizer')\n","  vectorizer.adapt(ds_train.take(1000).map(extract_text))\n","\n","  embedding_layer = layers.Embedding(vocab_size, embedding_size)\n","\n","  input = layers.Input(shape=(1,), dtype=tf.string)\n","  x = vectorizer(input)\n","  x = embedding_layer(x)\n","  x = layers.Bidirectional( layers.LSTM(hidden_size, return_sequences=True) )(x)\n","  x = layers.Bidirectional( layers.LSTM(hidden_size) )(x)\n","  output = layers.Dense(num_classes, activation='softmax')(x)\n","\n","  model = tf.keras.models.Model(input, output)\n","  model.compile(\n","      loss=SparseCategoricalCrossentropy(),\n","      optimizer=Adam(),\n","      metrics=[SparseCategoricalAccuracy()]\n","  )\n","\n","  print(model.summary())\n","\n","  return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tA_9zvbiLKVo"},"outputs":[],"source":["bidir_lstm = build_bidirectional_two_layer_LSTM_model(128, 16, num_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSuRX4RMLWIT"},"outputs":[],"source":["bidir_lstm_history = bidir_lstm.fit(\n","    ds_train_opt,\n","    validation_data=ds_test_opt,\n","    epochs=5\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNJDLYaeLp29"},"outputs":[],"source":["plot_history(bidir_lstm_history)"]},{"cell_type":"markdown","metadata":{"id":"tau_VPIqL4ym"},"source":["***\n","\n","# Congratulations!\n","\n","You've learned how to represent **text as tensors**, use **word embeddings**, and to **model natural language** to train a wide range of models with increasing complexity - from **simple RNNs** to **bidirectional multilayer LSTMs**.\n","\n","Until now, we've focused on using RNNs to classify sequences of text. But they can handle many more tasks, such as **text generation** and machine translation &mdash; we'll consider those tasks in the next unit.\n","\n","***"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMeMDHVKKEK8/0pVD47sc2E","gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
