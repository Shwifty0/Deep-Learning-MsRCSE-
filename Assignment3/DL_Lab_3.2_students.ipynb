{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyO0YeYdNsj6p0vQdb4Le1d9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# DL Lab 3.2 - Semantic Word Embeddings\n","\n","Welcome to the DL Lab! In this lab, you will train a **word embedding** from scratch and investigate its interesting properties.\n","\n","## Today's Learning Objectives\n","\n","- Use **Tokenizers** for **word vectorization**.\n","- Train models containing **word embeddings**.\n","- Compute **word similarities** and retrieve similar and analogous words.\n","\n","***\n","\n","**Note**: Training DNNs is a computationally expensive process. Most of the computations can be parallelized very efficently, making them a perfect fit for GPU-acceleration. In order to enable a GPU for your Colab session, do the following steps:\n","- Click '*Runtime*' -> '*Change runtime type*'\n","- In the pop-up window for '*Hardware accelerator*', select '*GPU*'\n","- Click '*Save*'"],"metadata":{"id":"Uk0ZyxjPoGhL"}},{"cell_type":"markdown","source":["# 1 - Word Embeddings\n","\n","Word embeddings give us a way to use an efficient, **dense representation** in which **similar words** have a **similar encoding**. Importantly, we do not have to specify this encoding by hand!\n","\n","An embedding is a dense vector of floating point values. The length of the vector is a parameter you specify. The values of the embedding are trainable parameters, i.e., weights learned by the model during training. It is common to see word embeddings that are 8-dimensional for small datasets, up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but requires more data to learn.\n","\n","An intuitive way to think of an embedding is as lookup table. After the embeddings weights have been learned, we can encode each word by looking up the dense vector it corresponds to in the table."],"metadata":{"id":"GIb7P5LAeYgh"}},{"cell_type":"markdown","source":["# 2 - Using the Embedding Layer\n","\n","So much for the motivation. Let's get started and use embeddings!\n","\n","Keras makes it easy to use word embeddings by means of its [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n","\n","The first argument `input_dim` specifies the size of the vocabulary. The second argument `output_dim` is the dimensionality of the embeddings, hence the length of the dense vectors. The `output_dim` is a parameter you can tune and experiment with in the same way you would experiment with the number of neurons in a dense layer.\n","\n","**Task**: Initialize a layer for embedding words of a vocabulary of 1000 words into 5 dimensions."],"metadata":{"id":"rGP42DBSedlv"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"metadata":{"id":"91-DcRaNOwIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### START YOUR CODE HERE ###  (≈1 LOC)\n","embedding_layer =\n","### END YOUR CODE HERE ###"],"metadata":{"id":"jv6XhECqenjQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When you create an `Embedding` layer, the weights for the embedding are randomly initialized just as for any other layer. During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words as learned for the specific problem your model was trained on.\n","\n","The `Embedding` layer can be understood as a lookup table that maps from integer indices (denoting specific words) to dense vectors (their embeddings). Hence, passing a list of integers to an embedding layer, the result replaces each integer with its corresponding vector from the embedding table.\n","\n","**Task**: Pass a 1D numpy array of integers to the `embedding_layer` and print the result."],"metadata":{"id":"b5wgYr1wezI8"}},{"cell_type":"code","source":["### START YOUR CODE HERE ###  (≈2 LOC)\n","input_word_indices =\n","result =\n","### END YOUR CODE HERE ###\n","\n","print(\"input.shape:\", input_word_indices.shape, \"\\n\")\n","print(\"result:\", result.numpy(), \"\\n\")\n","print(\"result.shape:\", result.shape, \"\\n\")"],"metadata":{"id":"f5jDpMz6e_tJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The returned tensor has one more axis than the input and the embedding vectors are aligned along the new last axis. Hence, the shape of the embedded tensor is `(samples, sequence_length, embedding_size)`."],"metadata":{"id":"N-hpmzAigCJ3"}},{"cell_type":"markdown","source":["# 3 - Training Embeddings from Scratch\n","\n","You can actually train word embeddings in a model for solving a certain task, just as next word prediction. For this task, you will use the same text data as for the DL Lab 3.1, i.e., the **[AG_NEWS](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)** dataset containing news articles of 4 different categories.\n","\n","Execute the cell below for downloading and preprocessing the text data."],"metadata":{"id":"k6b_avxngpIX"}},{"cell_type":"code","source":["import tensorflow_datasets as tfds\n","\n","BATCHSIZE = 128\n","\n","dataset = tfds.load('ag_news_subset')\n","train_ds = dataset['train']\n","val_ds = dataset['test']\n","\n","classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n","num_classes = len(classes)\n","\n","def extract_text(x):\n","    return x['title'] + ' ' + x['description']\n","\n","def tupelize(x):\n","    return (extract_text(x), x['label'])\n","\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_ds_opt = train_ds.map(tupelize).cache().shuffle(1000).batch(BATCHSIZE).prefetch(AUTOTUNE)\n","val_ds_opt = val_ds.map(tupelize).cache().batch(1000).prefetch(AUTOTUNE)"],"metadata":{"id":"bzPU798efn_m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Task**: Initialize a word-level [`TextVectorization` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization). Use a maximum vocabulary size of 10'000 words and a max sequence length of 100. Then `adapt` the vectorizer on 10'000 samples of the `train_ds`."],"metadata":{"id":"liwVk9wYvZ1o"}},{"cell_type":"code","source":["from tensorflow.keras import layers\n","\n","max_vocab_size = 10000\n","max_sequence_length = 100\n","\n","### START YOUR CODE HERE ###  (≈2 LOC)\n","vectorizer =\n","vectorizer.adapt(train_ds.take( ).map(extract_text))\n","### END YOUR CODE HERE ###"],"metadata":{"id":"xEU80aeNi1k-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `TextVectorization.get_vocabulary` function provides the vocabulary:"],"metadata":{"id":"iRpfDj7FxOzJ"}},{"cell_type":"code","source":["# Get the unique words in the vocabulary\n","vocab = vectorizer.get_vocabulary()\n","\n","# Length of the vocabulary\n","vocab_size = len(vocab)\n","print(f\"Number of words in vocab: {vocab_size}\")\n","\n","# most common tokens (notice the [UNK] token for \"unknown\" words)\n","top_5_words = vocab[:5]\n","print(f\"Top 5 most common words: {top_5_words}\")\n","\n","# least common tokens\n","bottom_5_words = vocab[-5:]\n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"],"metadata":{"id":"JoldZhleokV9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function `build_embedding_bag_model` returns a simple classification model based on the average of the embedding vectors:"],"metadata":{"id":"OcEtYJp6xZGT"}},{"cell_type":"code","source":["from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","\n","def build_embedding_bag_model(vectorizer, vocab_size, embedding_size, num_classes):\n","\n","  input = layers.Input(shape=(1,), dtype=tf.string)\n","  x = vectorizer(input)\n","  x = layers.Embedding(vocab_size, embedding_size)(x)\n","  x = layers.GlobalAveragePooling1D()(x)\n","  x = layers.Dense(16, activation='relu')(x)\n","  output = layers.Dense(num_classes, activation='softmax')(x)\n","\n","  model = tf.keras.models.Model(input, output)\n","  model.compile(\n","      loss=SparseCategoricalCrossentropy(),\n","      optimizer=Adam(),\n","      metrics=[SparseCategoricalAccuracy()]\n","  )\n","\n","  print(model.summary())\n","\n","  return model\n"],"metadata":{"id":"odzo-6KLjD7z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's create one model and train it on the AG news dataset:"],"metadata":{"id":"6Sw24qHEx4eC"}},{"cell_type":"code","source":["# @title define `plot_history()`\n","from matplotlib import pyplot as plt\n","\n","def plot_history(history):\n","  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n","  ax1.plot(history.history['loss'], label='training')\n","  ax1.plot(history.history['val_loss'], label='validation')\n","  ax1.set_ylabel('Loss')\n","  ax1.set_yscale('log')\n","  if history.history.__contains__('lr'):\n","    ax1b = ax1.twinx()\n","    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n","    ax1b.set_yscale('log')\n","    ax1b.set_ylabel('Learning Rate', color='g')\n","  ax1.legend()\n","\n","  key = None\n","  for k in sorted(history.history.keys()):\n","    if 'acc' in k and not 'val_' in k:\n","      key = k\n","      break\n","  if key:\n","    ax2.plot(history.history[key], label='training')\n","    ax2.plot(history.history['val_'+key], label='validation')\n","    ax2.set_ylabel('Accuracy')\n","    ax2.set_xlabel('Epochs')\n","  plt.show()"],"metadata":{"cellView":"form","id":"B7NEHeYRlZ4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_size = 16\n","\n","bag_model = build_embedding_bag_model(vectorizer, max_vocab_size, embedding_size, num_classes)\n","\n","bag_history = bag_model.fit(\n","    train_ds_opt,\n","    validation_data=val_ds_opt,\n","    epochs=10\n",")\n","\n","plot_history(bag_history)"],"metadata":{"id":"Zn2nYXyCk9q3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_history(bag_history)"],"metadata":{"id":"Z5CmRxhDmHrJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4 - Operations on Embeddings\n","\n","Let's retrieve the learned word embedding.\n","This will be a matrix of shape `(vocab_size, embedding_size)`.\n","\n","**Task**: Use the `get_weights()` method on the correct layer of the `bag_model` to obtain the embedding matrix."],"metadata":{"id":"oNf17-rsyIdS"}},{"cell_type":"code","source":["### START YOUR CODE HERE ###\n","embedding_layer_index =\n","### END YOUR CODE HERE ###\n","\n","emb_matrix = bag_model.layers[embedding_layer_index].get_weights()[0]\n","print(emb_matrix.shape)"],"metadata":{"id":"hjr8_c_bmdvf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using the word indices along with the vocabulary defined by the vectorizer, we can now retrieve word embedding vectors from the embedding matrix.\n","\n","**Task**: Complete the function `get_embedding_vector` for returning the embedding vector of a given word `word_in`."],"metadata":{"id":"RW15ymKDy2lm"}},{"cell_type":"code","source":["def get_embedding_vector( word_in, vocabulary, embedding_matrix ):\n","\n","  if not word_in in vocabulary:\n","    print(f'WARNING: \"{word_in}\" not in vocabulary. Falling back to \"[UNK]\" token.')\n","    word_in = \"[UNK]\"\n","\n","  # Get the word index in the vocabulary\n","  word_idx = vocabulary.index(word_in)\n","\n","  ### START YOUR CODE HERE ###  (≈1 LOC)\n","  # Lookup the embedding vector\n","  emb_vec =\n","  ### END YOUR CODE HERE ###\n","\n","  return emb_vec"],"metadata":{"id":"0RqxTPT3m3p2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab = vectorizer.get_vocabulary()\n","\n","get_embedding_vector('news', vocab, emb_matrix)"],"metadata":{"id":"OVl-x2eJm7cH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.1 - Word Similarity\n","\n","To measure how similar two words are, we need a way to measure the degree of similarity between two embeddings vectors for the two words. Given two word vectors $u$ and $v$, cosine similarity is defined by the cosine of the angle $\\theta$ between the two vectors:\n","\n","$$\\text{CosineSimilarity(u, v)} = cos(\\theta) = \\frac {u \\cdot v} {\\Vert u \\Vert \\Vert v \\Vert} $$\n","\n","where $u \\cdot v$ is the dot product of two vectors, $\\Vert u \\Vert$ is the norm (or length) of the vector $u$, and $\\theta$ is the angle between $u$ and $v$.\n","\n","If $u$ and $v$ are very similar, their cosine similarity will be close to 1. If they are dissimilar, the cosine similarity will take a smaller value down to -1.\n","\n","**Note**: The norm of $u$ is defined as $ \\Vert u \\Vert = \\sqrt{\\sum_{i=1}^{n} u_i^2}$.\n","\n","**Task**: Complete the function `cosine_similarity`."],"metadata":{"id":"wNGFOhpj0DLy"}},{"cell_type":"code","source":["def cosine_similarity(u, v):\n","\n","  ### START YOUR CODE HERE ###  (≈3 LOC)\n","\n","  ### END YOUR CODE HERE ###\n","\n","  return cos_similarity"],"metadata":{"id":"wQawYHwOoARH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test some combinations:"],"metadata":{"id":"caEYtMTP0Jdt"}},{"cell_type":"code","source":["def print_pair_similarity(word_a, word_b):\n","  print(cosine_similarity(\n","      get_embedding_vector( word_a, vocab, emb_matrix ),\n","      get_embedding_vector( word_b, vocab, emb_matrix )\n","  ))\n","\n","print_pair_similarity(\"queen\", \"woman\")\n","print_pair_similarity(\"queen\", \"man\")\n","print_pair_similarity(\"queen\", \"king\")"],"metadata":{"id":"fwN63wTDoCfx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.2 - Similar Word Retrieval\n","\n","We can also use the word embedding for retrieval of $k$ semantically close words:"],"metadata":{"id":"DcQPZpi_0Qwx"}},{"cell_type":"code","source":["def closest_word(word_in, vocabulary, emb_matrix, top_k=5):\n","\n","  # Get embedding vector of input word\n","  word_in_emb = get_embedding_vector(word_in, vocabulary, emb_matrix)\n","\n","  # Compute similarities\n","  similarity = [ cosine_similarity(w_emb, word_in_emb) for w_emb in emb_matrix ]\n","\n","  # Top-k words having largest similarity\n","  idxs = np.argsort( similarity )[::-1][1:top_k+1]\n","\n","  return [ [vocabulary[i], similarity[i]] for i in idxs ]"],"metadata":{"id":"SjfgU2HupXqs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test some words:"],"metadata":{"id":"7XQVh8Em0aVw"}},{"cell_type":"code","source":["for word in ['president', 'phone', 'soccer', 'science']:\n","  print(word, closest_word(word, vocab, emb_matrix))\n","  print()"],"metadata":{"id":"YZqwDYezsLUC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.3 - Word Analogy\n","\n","Another interesting task is the \"Word analogy task\", where we complete the sentence \"`a` is to `b` as `c` is to __\". In detail, we are trying to find a word `d`, such that the associated word vectors $e_a$, $e_b$, $e_c$, $e_d$, are related as follows:\n","$$ e_b - e_a \\approx e_d - e_c. $$\n","\n","The similary between $ e_b - e_a $ and $ e_d - e_c $ is measured using cosine similarity."],"metadata":{"id":"fdCyk1pa0kFT"}},{"cell_type":"code","source":["def complete_analogy(word_a, word_b, word_c):\n","\n","  # Convert words to lower case\n","  word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n","\n","  # Get the embedding vectors\n","  e_a = get_embedding_vector(word_a, vocab, emb_matrix)\n","  e_b = get_embedding_vector(word_b, vocab, emb_matrix)\n","  e_c = get_embedding_vector(word_c, vocab, emb_matrix)\n","\n","  max_cosine_sim = -100\n","  best_word = None\n","\n","  # Loop over the whole word vector set\n","  for w_idx, w in enumerate(emb_matrix):\n","\n","    # To avoid best_word being one of the input words, pass on them.\n","    if (w == [e_a, e_b, e_c]).all(1).any():\n","      continue\n","\n","    # Compute cosine similarity between the vector (e_b - e_a) and the vector (w - e_c)\n","    ### START YOUR CODE HERE ###  (≈1 LOC)\n","    cosine_sim = cosine_similarity( , )\n","    ### END YOUR CODE HERE ###\n","\n","    if cosine_sim > max_cosine_sim:\n","      # Set new max similarity\n","      max_cosine_sim = cosine_sim\n","      # Select new best_word\n","      best_word_idx = w_idx\n","\n","      print(cosine_sim, vocab[best_word_idx])\n","\n","  return vocab[best_word_idx]"],"metadata":{"id":"146dmyvE0brP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(complete_analogy('man', 'king', 'woman'))"],"metadata":{"id":"I8QXO0iH06j7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(complete_analogy('germany', 'german', 'china'))"],"metadata":{"id":"KL-xYtOs08ZJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You may try different word combinations. However, our word embedding is not very powerful as we used a rather simple model for training, and more importantly, a very small text corpus."],"metadata":{"id":"BaszPLDX1E6v"}},{"cell_type":"markdown","source":["## 4.4 - GloVe Embedding\n","\n","Execute the cell below to download a more powerful word embedding, i.e., a 50-dimensional GloVe word embedding trained on Wikipedia (2014) and the Gigaword 5 corpus. The [GloVe embedding](https://nlp.stanford.edu/projects/glove/) is provided by the NLP research group at Stanford University."],"metadata":{"id":"u399d8Ej3s6U"}},{"cell_type":"code","source":["#@title Download GloVe Embedding\n","\n","import requests, os, zipfile\n","import numpy as np\n","\n","data_path = '/tmp/glove'\n","glove_file = os.path.join(data_path, 'glove.6B.50d.txt')\n","!rm -rf $data_path\n","os.makedirs(data_path)\n","\n","# download glove file\n","!wget -nv -t 0 --show-progress -O $glove_file 'https://cloud.tu-ilmenau.de/s/m558re2RpoW8X2s/download/glove.6B.50d.txt'\n","!sleep 1\n","\n","def read_glove_vecs(glove_file):\n","  with open(glove_file, 'r') as f:\n","    words = set()\n","    word_to_vec_map = {}\n","\n","    for line in f:\n","      line = line.strip().split()\n","      curr_word = line[0]\n","      words.add(curr_word)\n","      word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n","\n","  return words, word_to_vec_map"],"metadata":{"cellView":"form","id":"XP4ussQ51dED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words, word_to_vec_map = read_glove_vecs(glove_file)"],"metadata":{"id":"CI_c_ii_1evz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can get the embedding vector of a string by a lookup in the `word_to_vec_map`. Let's compute some word similarities using the GloVe embedding:"],"metadata":{"id":"q4Ox4CAU349j"}},{"cell_type":"code","source":["def print_pair_similarity_glove(word_a, word_b):\n","  print(cosine_similarity(\n","      word_to_vec_map[word_a],\n","      word_to_vec_map[word_b]\n","  ))\n","\n","print_pair_similarity_glove(\"queen\", \"woman\")\n","print_pair_similarity_glove(\"queen\", \"man\")\n","print_pair_similarity_glove(\"queen\", \"king\")"],"metadata":{"id":"acLUx9-Y105M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see if the GloVe Embedding allows for better analogies:"],"metadata":{"id":"mTBfic5a37UV"}},{"cell_type":"code","source":["def complete_analogy_glove(word_a, word_b, word_c):\n","\n","  # Convert words to lower case\n","  word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n","\n","  # Get the embedding vectors\n","  e_a = word_to_vec_map[word_a]\n","  e_b = word_to_vec_map[word_b]\n","  e_c = word_to_vec_map[word_c]\n","\n","  words = word_to_vec_map.keys()\n","  max_cosine_sim = -100\n","  best_word = None\n","\n","  # Loop over the whole word vector set\n","  for w in words:\n","\n","    # To avoid best_word being one of the input words, pass on them.\n","    if w in [word_a, word_b, word_c] :\n","      continue\n","\n","    # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)\n","    cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n","\n","    if cosine_sim > max_cosine_sim:\n","      # Set new max similarity\n","      max_cosine_sim = cosine_sim\n","      # Select new best_word\n","      best_word = w\n","\n","  return best_word"],"metadata":{"id":"4QpI73Fy2cKm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Try out a few! Nice triplets are\n","- `('king', 'man', 'queen')`\n","- `('germany', 'german', 'china')`\n","- `('india', 'delhi', 'japan')`\n","- `('man', 'woman', 'boy')`"],"metadata":{"id":"XGrQLvSD2tsn"}},{"cell_type":"code","source":["#complete_analogy_glove('king', 'man', 'queen')\n","complete_analogy_glove('germany', 'german', 'china')\n","#complete_analogy_glove('india', 'delhi', 'japan')\n","#complete_analogy_glove('man', 'woman', 'boy')"],"metadata":{"id":"q092rVFU2mXu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And for the most similar words:"],"metadata":{"id":"ziJGSiA-3_To"}},{"cell_type":"code","source":["def closest_word_glove(embedding_vector, remove_words=[], top_k=5):\n","\n","  # Get vocabulary\n","  vocabulary = list(word_to_vec_map.keys())\n","  # To avoid top words being one of the input words, remove them from list\n","  for w in remove_words:\n","    vocabulary.remove(w)\n","\n","  # Compute embeddings of all words\n","  w_embeddings = np.array([word_to_vec_map[w] for w in vocabulary])\n","\n","  # Compute similarities\n","  similarity = [ cosine_similarity(w_emb, embedding_vector) for w_emb in w_embeddings ]\n","\n","  # Index of max similary\n","  idxs = np.argsort( similarity )[::-1][:top_k]\n","\n","  return [ [vocabulary[i], similarity[i]] for i in idxs ]\n","\n","\n","for word in ['president', 'phone', 'soccer', 'science']:\n","  print(closest_word_glove(word_to_vec_map[word], [word]))\n","  print()"],"metadata":{"id":"9LA1szeY2qj2"},"execution_count":null,"outputs":[]}]}