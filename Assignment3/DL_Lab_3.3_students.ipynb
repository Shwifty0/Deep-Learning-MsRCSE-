{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1RggCfYSyPSfXLnYevLdOrnMi4kdnHnRI","timestamp":1607502671645}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AOqpfpFheJgO"},"source":["# DL Lab 3.3 - Text Classification with Transformers\n","\n","Welcome to the DL Lab! Discussing sequence processing, you heard a lot about Recurrent Neural Networks (RNNs) and their different architectures. At last, one of the most breakthrough architectural mechanisms was discussed: the **self-attention** mechanism, which represents the key concept for the so called **Transformer** architecture.\n","\n","In this homework, you will implement an **encoder** based on **multi-head attention** and use it for **text classification**.\n","\n","***\n","\n","**After completing this homework you will be able to**\n","\n","- Use TF's **subclassing** API to efficiently create **custom layers** and **blocks**.\n","- Implement and use **scaled dot product attention** in **multiple attention heads**.\n","\n","***"]},{"cell_type":"markdown","metadata":{"id":"ff9-wGriF5yW"},"source":["# 1 - Data Preparation\n","\n","you will use the same text data as for the DL Lab 3.1, i.e., the **[AG_NEWS](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)** dataset containing news articles of 4 different categories.\n","\n","Execute the cell below for downloading and preprocessing the text data."]},{"cell_type":"code","metadata":{"id":"DnOgVmq62_qI"},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","BATCHSIZE = 32\n","\n","dataset = tfds.load('ag_news_subset')\n","train_ds = dataset['train']\n","val_ds = dataset['test']\n","\n","classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n","num_classes = len(classes)\n","\n","def extract_text(x):\n","    return x['title'] + ' ' + x['description']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We also need a word-level vectorizer to convert the text into a numerical representation:"],"metadata":{"id":"yRM5cjZR3-dY"}},{"cell_type":"code","source":["from tensorflow.keras import layers, Model\n","\n","max_vocab_size = 10000\n","max_sequence_length = 100\n","\n","vectorizer = layers.TextVectorization(\n","    max_tokens = max_vocab_size,\n","    output_sequence_length = max_sequence_length\n",")\n","vectorizer.adapt(train_ds.take(1000).map(extract_text))"],"metadata":{"id":"KC7zvXGk37gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the unique words in the vocabulary\n","vocab = vectorizer.get_vocabulary()\n","\n","# Length of the vocabulary\n","vocab_size = len(vocab)\n","print(f\"Number of words in vocab: {vocab_size}\")\n","\n","# most common tokens (notice the [UNK] token for \"unknown\" words)\n","top_5_words = vocab[:5]\n","print(f\"Top 5 most common words: {top_5_words}\")\n","\n","# least common tokens\n","bottom_5_words = vocab[-5:]\n","print(f\"Bottom 5 least common words: {bottom_5_words}\")"],"metadata":{"id":"aJcZH6Rk4L8c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's tokenize our text and optimize it for performance:"],"metadata":{"id":"LQXAbCWs9ann"}},{"cell_type":"code","source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","def tupelize(x):\n","    return (vectorizer(extract_text(x)), x['label'])\n","\n","train_ds_opt = train_ds.map(tupelize)\n","train_ds_opt = train_ds_opt.cache().shuffle(1000).batch(BATCHSIZE).prefetch(AUTOTUNE)\n","\n","val_ds_opt = val_ds.map(tupelize)\n","val_ds_opt = val_ds_opt.cache().batch(BATCHSIZE).prefetch(AUTOTUNE)"],"metadata":{"id":"xwThGQL29ZuV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EtDe8fNagLif"},"source":["# 2 - Transformers\n","\n","Before the rise of Transformer models, the state of the art in any sequence related tasks (translation, image captioning, ...) was basically a RNN with added attention mechanism (compare with Lecture 3.2 - Attention).\n","\n","Transformer architectures also rely on attention mechanisms, but without any recurrent processing. The impactful [*Attention is all you need*](https://arxiv.org/abs/1706.03762) paper highlights that the attention mechanism alone is as powerful as RNNs with added attention. However, recurrent processing requires that sequences are processed strictly in order both during forward and backward propagation. In Transformers, a sequence can be processed *at once*, which significantly reduces training times and allows for much larger datasets to be efficiently processed."]},{"cell_type":"markdown","metadata":{"id":"-i0PW1TOGYWw"},"source":["## 2.1 - Define Multi-head Attention Encoder\n","\n","The heart of each transformer is an encoder-decoder structure. The encoder processes the input sequence to generate encodings of each token in such way that they contain contextual information, i.e., the encoder encodes which tokens of the input sequence are relevant to each other. The decoder part then transforms these token encodings to generate an output sequence.\n","\n","### Scaled Dot-Product Attention\n","\n","In order to compute this contextual token relevancy, both the encoder and the decoder are using a specific type of attention: the **scaled dot-product attention**, which allows for dynamic computation of the weighted average of the features of the input tokens. \"Dynamic\" means that the weights are not pre-defined but depend on the actual values of the tokens.\n","\n","Therefore, for each token embedding $x_i \\in X$ of the input sequence, a set of vectors is computed: a **query** vector $q_i$, a **key** vector $k_i$, and a **value** vector $v_i$. These vectors are computed by linear projection using trainable weight matrices:\n","$$\n","q_i = x_i W_q\\\\\n","k_i = x_i W_k\\\\\n","v_i = x_i W_v.\n","$$\n","\n","As you can see, computing these vectors is exactly the same as computing the output of a fully connected layer. You will also implement it as such. ;-)\n","\n","The attention weight\n","\n","$$a_{ij} = q_i \\cdot k_j$$\n","\n","from token $i$ to token $j$ is based on the similarity between the query vector $q_i$ of token $i$ and the key vector $k_j$ of token $j$, and the dot product is used as similarity metric. Hence, the attention weights for all tokens can be computed by matrix-matrix multiplication $Q K^T$, where Q (K) contains the query (key) vectors for all input tokens of the sequence. In addition, we normalize the attention weights so that they are between $0$ and $1$ and sum up to $1$. As the average value of the dot product increases with increasing embedding dimension $d_k$, we also divide by $\\sqrt{d_k}$ to mitigate the amount by which the increase in dimension increases the dot product.\n","\n","Ultimately, the hidden representations of the tokens with attention is computed by multiplying the scaled attention weights with the value vectors $V$:\n","\n","$$H(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}}\\right)V$$.\n","\n","<img src='https://3.bp.blogspot.com/-mnlTQLXKuiU/XfgOSZ2eBsI/AAAAAAAAB0w/6jjXEtzO6_M1IlPkNVzR_wcmP62u0jI0ACLcBGAsYHQ/s1600/attention.png' width=\"760\">\n","\n","### Multi-head Attention\n","\n","Having multiple attention heads, each computing the scaled dot-product attention as described above, the model can learn different relevance relations between the input tokens. The output of all heads is concatenated and propagated through a linear layer reducing the dimension to the hidden size of the model. Each head will process a different chunk of the token embedding. To speed up computation, we first compute the $Q, K, V$ matrices and then split and distribute them across the different heads. You can think of it as stacking all the $W_q^{(h)}, W_k^{(h)}, W_v^{(h)}$ weight matrices of the $h$ different heads into combined weight matrices $W_q, W_k, W_v$.\n"]},{"cell_type":"markdown","metadata":{"id":"Z0oRV2lnx-H2"},"source":["**Task**: Complete the `MultiHeadSelfAttention` class defined below.\n","- Initialize the [dense layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) `query_linear`, `key_linear`, `value_linear` for computing the linear transformations of the inputs. Each layer shall have `hidden_size` neurons, use linear activation and no bias.\n","- Compute the dot product attention in the `attention` method using [`tf.matmul()`](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul)."]},{"cell_type":"code","metadata":{"id":"xeO3COio2DJn"},"source":["class MultiHeadSelfAttention(layers.Layer):\n","  ''' Multi-head self attention layer consisting of four parts:\n","    - linear layers and separation into multiple heads\n","    - scaled dot product attention\n","    - concatenation of multiple heads\n","    - final linear layer\n","  '''\n","\n","  def __init__(self, hidden_size, num_heads=4):\n","    assert hidden_size % num_heads == 0, \\\n","      'The hidden size (={}) has to be divisble by number of heads (={})'.format(\n","          hidden_size, num_heads)\n","    super(MultiHeadSelfAttention, self).__init__()\n","    self.hidden_size = hidden_size\n","    self.num_heads = num_heads\n","    self.projection_dim = hidden_size // num_heads\n","\n","    # Layers for linearly projecting to queries, keys and values\n","    ### START YOUR CODE HERE ###  (3 LOC)\n","    self.query_linear =\n","    self.key_linear =\n","    self.value_linear =\n","    ### END YOUR CODE HERE ###\n","    self.output_linear = layers.Dense(hidden_size, name='transform_output')\n","\n","\n","  def attention(self, query, key, value):\n","    ''' Apply scaled dot product attention\n","\n","    Args:\n","      query : Tensor with shape (batch_size, num_heads, sequence_length, hidden_size/num_heads)\n","      key : Tensor with shape (batch_size, num_heads, sequence_length, hidden_size/num_heads)\n","      value : Tensor with shape (batch_size, num_heads, sequence_length, hidden_size/num_heads)\n","\n","    Returns:\n","      Tensor with shape (batch_size, num_heads, sequence_length, hidden_size/num_heads)\n","    '''\n","\n","    # compute dot product attention\n","    ### START YOUR CODE HERE ###  (1 LOC)\n","    logits =\n","    ### END YOUR CODE HERE ###\n","\n","    # scale logits\n","    dim_key = tf.cast( tf.shape(key)[-1], tf.float32 )\n","    logits = logits / tf.math.sqrt(dim_key)\n","\n","    # apply softmax\n","    attention_weights = tf.nn.softmax(logits, axis=-1, name='attention_weights')\n","\n","    # multiply attention weights with value\n","    output = tf.matmul(attention_weights, value)\n","\n","    return output\n","\n","\n","  def separate_heads(self, x, batch_size):\n","    ''' Separate x into different heads and transpose resulting value.\n","\n","    Args:\n","      x : Tensor with shape (batch_size, sequence_length, hidden_size)\n","\n","    Returns:\n","      Tensor with shape (batch_size, num_heads, sequence_length, hidden_size/num_heads)\n","    '''\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","    return tf.transpose(x, perm=[0,2,1,3])\n","\n","\n","  def combine_heads(self, x, batch_size):\n","    ''' Combine splitted tensor.\n","\n","    Args:\n","      x : Tensor with shape (batch_size, num_heads, sequence_length, hidden_size/num_heads)\n","\n","    Returns:\n","      Tensor with shape (batch_size, sequence_length, hidden_size)\n","    '''\n","    x = tf.transpose(x, perm=[0,2,1,3])\n","    return tf.reshape(x, (batch_size, -1, self.hidden_size))\n","\n","\n","  def call(self, inputs):\n","    ''' Apply self-attention mechanism to inputs.\n","\n","    Args:\n","      inputs : Tensor with shape (batch_size, sequence_length, hidden_size)\n","\n","    Returns:\n","      Tensor with shape (batch_size, sequence_length, hidden_size)\n","    '''\n","    # store batch_size\n","    batch_size = tf.shape(inputs)[0]\n","\n","    # transform q, k, v by linear projection\n","    query = self.query_linear(inputs)\n","    key = self.key_linear(inputs)\n","    value = self.value_linear(inputs)\n","\n","    # separate q, k, v into heads\n","    query = self.separate_heads(query, batch_size)\n","    key = self.separate_heads(key, batch_size)\n","    value = self.separate_heads(value, batch_size)\n","\n","    # apply self-attention\n","    attention = self.attention(query, key, value)\n","\n","    # combine heads\n","    attention = self.combine_heads(attention, batch_size)\n","\n","    # (batch_size, sequence_length, hidden_size)\n","    attention_output = self.output_linear(attention)\n","\n","    return attention_output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gsdPLYxKMjea"},"source":["Each encoder layer of the transformer consists of the multi-head attention layer, and two dense layers with dropout."]},{"cell_type":"code","metadata":{"id":"rFlPwzqb6zGs"},"source":["class EncoderLayer(layers.Layer):\n","  def __init__(self, embedding_dim, ff_dim, num_heads=4, rate=.1):\n","    super(EncoderLayer, self).__init__()\n","    self.attention = MultiHeadSelfAttention(embedding_dim, num_heads)\n","    self.ffn = tf.keras.Sequential( [layers.Dense(ff_dim, activation='relu'),\n","                                     layers.Dense(embedding_dim)] )\n","    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","\n","  def call(self, inputs, training):\n","    attn_output = self.attention(inputs)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out = self.layernorm1(inputs + attn_output)\n","    ffn_output = self.ffn(out)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    return self.layernorm2(out + ffn_output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHcZj7_yRL5w"},"source":["Unlike RNNs, the transformer model has no meaning of sequences of operations for a set of inputs. Hence it is important to keep track of the order of inputs. You can either train an *positional embedding* using the `EmbeddingWithPositionalEmbedding` layer below (the model then learns to encode positions), or use the *positional encoding* layer `EmbeddingWithPositionalEncoding`. Both should perform equally well."]},{"cell_type":"code","metadata":{"id":"hwOjilwb8do5"},"source":["class EmbeddingWithPositionalEmbedding(layers.Layer):\n","  def __init__(self, max_length, vocabulary_size, embedding_dim):\n","    super(EmbeddingWithPositionalEmbedding, self).__init__()\n","    self.token_emb = layers.Embedding(vocabulary_size, embedding_dim)\n","    self.position_emb = layers.Embedding(max_length, embedding_dim)\n","\n","  def call(self, x):\n","    max_length = tf.shape(x)[-1]\n","    positions = tf.range(start=0, limit=max_length, delta=1)\n","    positions = self.position_emb(positions)\n","    x = self.token_emb(x)\n","    return x + positions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVE40qAfLvcS"},"source":["class EmbeddingWithPositionalEncoding(layers.Layer):\n","  def __init__(self, max_length, vocabulary_size, embedding_dim):\n","    super(EmbeddingWithPositionalEncoding, self).__init__()\n","    self.embedding_dim = embedding_dim\n","    self.vocabulary_size = vocabulary_size\n","    self.token_emb = layers.Embedding(vocabulary_size, embedding_dim)\n","    self.pos_encoding = self.positional_encoding(vocabulary_size)\n","\n","  def _get_angles(self, position, i):\n","    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(self.embedding_dim, tf.float32))\n","    return position * angles\n","\n","  def positional_encoding(self, position):\n","    angle_rads = self._get_angles(\n","        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n","        i=tf.range(self.embedding_dim, dtype=tf.float32)[tf.newaxis, :]\n","        )\n","    # apply sin to even index in the array\n","    sines = tf.math.sin(angle_rads[:, 0::2])\n","    # apply cos to odd index in the array\n","    cosines = tf.math.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = tf.concat([sines, cosines], axis=-1)\n","    pos_encoding = pos_encoding[tf.newaxis, ...]\n","    return tf.cast(pos_encoding, tf.float32)\n","\n","  def call(self, x):\n","    x = self.token_emb(x)\n","    positions = self.pos_encoding[:, :tf.shape(x)[1], :]\n","    return x + positions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSWU1GxBGU4o"},"source":["## 2.2 - Create Model\n","\n","Now you can use the custom layers to build your own transformer encoder:"]},{"cell_type":"code","metadata":{"id":"zINr8712Gflk"},"source":["from tensorflow.keras import layers, Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","\n","def build_transformer_encoder(\n","    embedding_dim,\n","    num_heads,\n","    ff_dim,\n","    max_length,\n","    vocabulary_size,\n","    learning_rate=1e-3,\n","    summary=True\n","    ):\n","\n","  input_ = layers.Input( shape=(max_length,) )\n","\n","  # choose one\n","  #embedding_layer = EmbeddingWithPositionalEmbedding(max_length, vocabulary_size, embedding_dim)\n","  embedding_layer = EmbeddingWithPositionalEncoding(max_length, vocabulary_size, embedding_dim)\n","  x = embedding_layer(input_)\n","\n","  transformer_block = EncoderLayer(embedding_dim, ff_dim, num_heads)\n","\n","  x = transformer_block(x)\n","  x = layers.GlobalAveragePooling1D()(x)\n","  x = layers.Dropout(.1)(x)\n","  x = layers.Dense(x.shape[1], activation='relu')(x)\n","  x = layers.Dropout(.1)(x)\n","  output_ = layers.Dense(4, activation='softmax')(x)\n","\n","  model = Model(input_, output_)\n","\n","  model.compile(\n","      loss=SparseCategoricalCrossentropy(),\n","      optimizer=Adam(\n","          learning_rate=learning_rate,\n","          beta_2=0.98,\n","          epsilon=1e-9\n","      ),\n","      metrics=SparseCategoricalAccuracy()\n","  )\n","\n","  if summary:\n","    print(model.summary())\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BrAvLZvzpJxl"},"source":["Training transformers is typically done using learning rate warm-up. Instead of making large updates from the very beginning, we start to learn slowly by gradually increasing the learning rate from 0 to a certain value. After this warm-up, the learning rate decays again.\n","\n","The [*Attention is all you need* paper](https://arxiv.org/abs/1706.03762) provides following formula, which we implement as custom learning rate scheduler:\n","\n","$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$."]},{"cell_type":"code","metadata":{"id":"w3jIwz7ipHf9"},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","\n","  def __init__(self, d_model, warmup_steps=200):\n","    super(CustomSchedule, self).__init__()\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    step = tf.cast(step, tf.float32)\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k1X38J47c8eW"},"source":["Check the scheduled learning rate:"]},{"cell_type":"code","metadata":{"id":"2qQtjiu4uEGd"},"source":["from matplotlib import pyplot as plt\n","\n","EMBEDDING_DIM = 32\n","\n","LearningRateSchedule = CustomSchedule(EMBEDDING_DIM)\n","\n","plt.plot(LearningRateSchedule(tf.range(2000, dtype=tf.float32)))\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Train Step\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6F1sBVdJIoxn"},"source":["## 2.3 - Train and Evaluate Model\n","\n","Finally, you can train your transformer! If everything works well, the model should immediately converge and start overfitting after only few epochs."]},{"cell_type":"code","metadata":{"id":"GZ7AR_QgIn2g"},"source":["# @title define `plot_history()`\n","from matplotlib import pyplot as plt\n","import numpy as np\n","\n","def plot_history(history):\n","  fig, (ax1, ax2) = plt.subplots(2,1, sharex=True, dpi=150)\n","  ax1.plot(history.history['loss'], label='training')\n","  ax1.plot(history.history['val_loss'], label='validation')\n","  ax1.set_ylabel('Loss')\n","  ax1.set_yscale('log')\n","  if history.history.__contains__('lr'):\n","    ax1b = ax1.twinx()\n","    ax1b.plot(history.history['lr'], 'g-', linewidth=1)\n","    ax1b.set_yscale('log')\n","    ax1b.set_ylabel('Learning Rate', color='g')\n","  ax1.legend()\n","\n","  key = None\n","  for k in sorted(history.history.keys()):\n","    if 'acc' in k and not 'val_' in k:\n","      key = k\n","      break\n","  if key:\n","    ax2.plot(history.history[key], label='training')\n","    ax2.plot(history.history['val_'+key], label='validation')\n","    ax2.set_ylabel('Accuracy')\n","    ax2.set_xlabel('Epochs')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"COYzM4kUKKXk"},"source":["num_heads = 4 # Number of attention heads\n","ff_dim = 16 # Hidden layer size in feed forward network inside transformer\n","\n","BATCHSIZE = 32\n","\n","LearningRateSchedule = CustomSchedule(EMBEDDING_DIM)\n","\n","EarlyStoppingCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n","                                                         patience=3,\n","                                                         restore_best_weights=True)\n","\n","my_transformer = build_transformer_encoder(\n","    EMBEDDING_DIM, num_heads, ff_dim,\n","    max_sequence_length,\n","    vocab_size,\n","    learning_rate=LearningRateSchedule\n",")\n","\n","history = my_transformer.fit(\n","    train_ds_opt,\n","    batch_size=BATCHSIZE,\n","    epochs=10,\n","    validation_data=val_ds_opt,\n","    callbacks=[EarlyStoppingCallback]\n",")\n","\n","plot_history(history)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_history(history)"],"metadata":{"id":"gOe5OMRbA0dn"},"execution_count":null,"outputs":[]}]}